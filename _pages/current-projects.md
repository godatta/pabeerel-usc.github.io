---
permalink: /current-projects/
toc: true
---
## Current Projects

### Agile Hardware/Software Design Methodology

#### Design Space Exploration of CGRA Processing Elements using Peak DSL
**Kathleen Feng**

#### Auto-scheduling Image Processing and DDN Workloads on CGRAs
**Adam Dai**

#### Unified Buffer Halide Compiler and Auto-scheduler
Jeff, Dillon, Adam

#### Lake: Memory DSL
Qiaoyi, Max, Keyi, Taeyoung, Kavya

#### Gemstone: Physical Design Generator
Alex, Raj, Taeyoung, James

#### SoC Generation Framework
Gedeon

#### End-to-end Application Execution
Teguh, Gedeon

### Accelerator Architectures Leveraging Emerging Technologies

#### 3D CGRA Architecture with Hybrid RRAM-NEMS-based Interconnect
**Akash Levy**

#### One-Shot Learning using RRAM-Based Associative Memory
**Haitong Li**

#### Chimera: Compute (Immersed) in Memory with Embedded Resistive Arrays
**Kartik Prabhu, Rohan Doshi**
The scale of deep neural networks (DNNs) trained on increasingly large datasets has rapidly outpaced the amount of memory that can be densely integrated on the same die as compute in conventional CMOS technology. This has created an energy and performance bottleneck at the interface with off-chip DRAM. With a theoretically 12F2 1T1R cell, multi-level capability, and the promise of monolithic 3D integration, resistive RAM (RRAM) offers a new, orthogonal path to significant strides in energy-efficiency and performance of DNN hardware. Driven bottom-up by this emerging technology, we propose a set of DNN hardware architectures which leverage the density of RRAM in both the near-memory, digital and in-memory, mixed-signal computing contexts.

#### In-Memory Computing Architecture for Probabilistic Graphical Models
**Weier Wan**
